# chat-13b
model           =   ggml-llama-q4_0-ggjt.bin
color           =   
batch_size      =   1024
n_predict       =   2048
ctx_size        =   2048
top_k           =   40
top_k           =   0.5
repeat_last_n   =   256
temp            =   0.7
repeat_penalty  =   1.17647
threads         =   12
interactive     =   
file            =   chat-13b.txt
keep            =   48
reverse-prompt  =   'Tom:'