# chat
model           =   ggml-llama-q4_0-ggjt.bin
color           =   
batch_size      =   1024
n_predict       =   256
ctx_size        =   512
top_k           =   10000
temp            =   0.2
repeat_penalty  =   1.0
threads         =   12
interactive     =   
file            =   chat-with-bob.txt
keep            =   48